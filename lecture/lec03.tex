\documentclass{beamer}

% Roofline: 21.3 GB/s max memory bandwidth
% 64 GFlop/s theoretical peak

\mode<presentation>
{
%  \usetheme[hideothersubsections]{PaloAlto}
  \usetheme{default}
  \setbeamercovered{transparent}
}

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}

\usepackage{amsmath,amssymb}
\usepackage{times} 
\usepackage[T1]{fontenc}

\title[CS 5220, Spring 2014]{Lecture 3: \\
  Single processor architecture and memory} 

\author[]{David Bindel} \date[]{30 Jan 2014}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}


\begin{frame}
  \frametitle{Logistics}

  \begin{itemize}
  \item Raised enrollment from 75 to 94 last Friday.
  \item Current enrollment is 90; C4 and CMS should be current?
  \item HW 0 (getting to know C4) due Tuesday.
  \item HW 1 (tuning matrix multiply) out Tuesday.
    \begin{itemize}
    \item Teams of 2--3 (or 2--4)
    \item Look for complementary groups!
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Just for fun}
  
  \begin{center}
  \url{http://www.youtube.com/watch?v=fKK933KK6Gg}
  \end{center}

  \begin{center}
    Is this a fair portrayal of your CPU? \\[5mm]
    (See Rich Vuduc's talk, 
    \href{http://web.eecs.utk.edu/~dongarra/ccgsc2010/slides/talk27-vuduc.pdf}
         {``Should I port my code to a GPU?''})
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{The idealized machine}
  
  \begin{center}
    \includegraphics[width=0.4\textwidth]{smiley.pdf}
  \end{center}

  \begin{itemize}
  \item Address space of named words
  \item Basic operations are register read/write, logic, arithmetic
  \item Everything runs in the program order
  \item High-level language translates into ``obvious'' machine code
  \item All operations take about the same amount of time
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The real world}

  \begin{center}
    \includegraphics[width=0.22\textwidth]{monalisa.jpg}
  \end{center}

  \begin{itemize}
  \item Memory operations are {\em not} all the same!
    \begin{itemize}
    \item Registers and caches lead to variable access speeds
    \item Different memory layouts dramatically affect performance
    \end{itemize}
  \item Instructions are non-obvious!
    \begin{itemize}
    \item Pipelining allows instructions to overlap
    \item Functional units run in parallel (and out of order)
    \item Instructions take different amounts of time
    \item Different costs for different orders and instruction mixes
    \end{itemize}
  \end{itemize}

  Our goal: enough understanding to help the compiler out.
\end{frame}

\begin{frame}
  \frametitle{Prelude}

  We hold these truths to be self-evident:
  \begin{enumerate}
  \item One should not sacrifice correctness for speed
  \item One should not re-invent (or re-tune) the wheel
  \item Your time matters more than computer time
  \end{enumerate}
  
  Less obvious, but still true:
  \begin{enumerate}
  \item Most of the time goes to a few bottlenecks
  \item The bottlenecks are hard to find without measuring
  \item Communication is expensive (and often a bottleneck)
  \item A little good hygiene will save your sanity
    \begin{itemize}
    \item Automate testing, time carefully, and use version control
    \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{A sketch of reality}

  Today, a play in two acts:%
\footnote{%
If you don't get the reference to {\em This American Life},
go find the podcast!}
  \begin{enumerate}
  \item Act 1: One core is not so serial
  \item Act 2: Memory matters
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Act 1}
  One core is not so serial.
\end{frame}

\begin{frame}
  \frametitle{Parallel processing at the laundromat}

  \begin{itemize}
  \item Three stages to laundry: wash, dry, fold.
  \item Three loads: {\color{blue} darks}, {\color{red} lights}, {\color{green} underwear}
  \item How long will this take?
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Parallel processing at the laundromat}

  \begin{itemize}
  \item Serial version: \\[2mm]
        \begin{tabular}{lllllllll}
          1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\ \hline
          {\color{blue} wash} & {\color{blue} dry} & {\color{blue} fold} & & & & & & \\
          & & & {\color{red} wash} & {\color{red} dry} & {\color{red} fold} & & & \\
          & & & & & & {\color{green} wash} & {\color{green} dry} & {\color{green} fold} \\ \hline
        \end{tabular}
  \item Pipeline version: \\[2mm]
        \begin{tabular}{lllll|l}
          1 & 2 & 3 & 4 & 5 \\ \hline
          {\color{blue} wash} & {\color{blue} dry} & {\color{blue} fold} & & & Dinner? \\
          & {\color{red} wash} & {\color{red} dry} & {\color{red} fold} & & Cat videos? \\
          & & {\color{green} wash} & {\color{green} dry} & {\color{green} fold} & Gym and tanning? \\ \hline
        \end{tabular}

  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Pipelining}
  
  \begin{itemize}
  \item Pipelining improves {\em bandwidth}, but not {\em latency}
  \item Potential speedup = number of stages
    \begin{itemize}
    \item But what if there's a branch?
    \end{itemize}
  \item Different pipelines for different functional units
    \begin{itemize}
    \item Front-end has a pipeline
    \item Functional units (FP adder, FP multiplier) pipelined
    \item Divider is frequently not pipelined
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{SIMD}
  
  \begin{itemize}
  \item {\em S}ingle {\em I}nstruction {\em M}ultiple {\em D}ata
  \item Old idea had a resurgence in mid-late 90s (for graphics)
  \item Now short vectors are ubiquitous...
    \begin{itemize}
    \item C4 instructional: 128 bits (two doubles) in a vector (SSE4.2)
    \item Newer CPUs: 256 bits (four doubles) in a vector (AVX)
    \item And then there are GPUs!
    \end{itemize}
  \item Alignment often matters
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Example: My laptop}

  MacBook Pro 15 in, early 2011.
  \begin{itemize}
  \item Intel Core i7-2635QM CPU at 2.0 GHz.  4 core / 8 thread.
  \item AVX units provide up to 8 double flops/cycle \\
        (Simultaneous vector add + vector multiply)
  \item Wide dynamic execution: up to four full instructions at once
  \item Operations internally broken down into ``micro-ops''
    \begin{itemize}
    \item Cache micro-ops -- like a hardware JIT?!
    \end{itemize}
  \end{itemize}
  Theoretical peak: 64 GFlop/s?

\end{frame}

\begin{frame}
  \frametitle{Punchline}
  
  \begin{itemize}
  \item Special features: SIMD instructions, maybe FMAs, ...
  \item Compiler understands how to utilize these {\em in principle}
    \begin{itemize}
    \item Rearranges instructions to get a good mix
    \item Tries to make use of FMAs, SIMD instructions, etc
    \end{itemize}
  \item In practice, needs some help:
    \begin{itemize}
    \item Set optimization flags, pragmas, etc
    \item Rearrange code to make things obvious and predictable
    \item Use special intrinsics or library routines
    \item Choose data layouts, algorithms that suit the machine
    \end{itemize}
  \item Goal: You handle high-level, compiler handles low-level.
  \end{itemize}
    
\end{frame}

\begin{frame}
  \frametitle{Act 2}
  Memory matters.
\end{frame}


\begin{frame}
  \frametitle{Roofline model}

  S. Williams, A. Waterman, D. Patterson,
  ``\href{http://www.eecs.berkeley.edu/Pubs/TechRpts/2008/EECS-2008-134.pdf}{Roofline:
    An Insightful Visual Performance Model for Floating-Point Programs
    and Multicore Architectures},'' CACM, April 2009.
\end{frame}


\begin{frame}
  \frametitle{My machine}

  \begin{itemize}
  \item Theoretical peak flop rate: 64 GFlop/s
  \item Peak memory bandwidth: 21.3 GB/s
  \item Arithmetic intensity = flops / memory accesses
  \item Example: Sum several million doubles (AI = 1) -- how fast?
  \item So what can we do?  Not much if lots of fetches, but...
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Cache basics}
  
  Programs usually have {\em locality}
  \begin{itemize}
  \item {\em Spatial locality}: things close to each other tend to be
    accessed consecutively
  \item {\em Temporal locality}: use a ``working set'' of data repeatedly
  \end{itemize}
  Cache hierarchy built to use locality.

\end{frame}


\begin{frame}
  \frametitle{Cache basics}

  \begin{itemize}
  \item Memory {\em latency} = how long to get a requested item
  \item Memory {\em bandwidth} = how fast memory can provide data
  \item Bandwidth improving faster than latency
  \end{itemize}

  Caches help:
  \begin{itemize}
  \item Hide memory costs by reusing data
    \begin{itemize}
    \item Exploit temporal locality
    \end{itemize}
  \item Use bandwidth to fetch a {\em cache line} all at once
    \begin{itemize}
    \item Exploit spatial locality
    \end{itemize}
  \item Use bandwidth to support multiple outstanding reads
  \item Overlap computation and communication with memory
    \begin{itemize}
    \item Prefetching
    \end{itemize}
  \end{itemize}

  This is mostly automatic and implicit.

\end{frame}


\begin{frame}
  \frametitle{Cache basics}

  \begin{itemize}
  \item Store cache {\em line}s of several bytes
  \item Cache {\em hit} when copy of needed data in cache
  \item Cache {\em miss} otherwise.  Three basic types:
    \begin{itemize}
    \item {\em Compulsory} miss: never used this data before
    \item {\em Capacity} miss: filled the cache with other things
      since this was last used -- working set too big
    \item {\em Conflict} miss: insufficient associativity for access
      pattern
    \end{itemize}
    \item {\em Associativity}
      \begin{itemize}
      \item Direct-mapped: each address can only go in one cache location
        (e.g. store address xxxx1101 only at cache location 1101)
      \item $n$-way: each address can go into one of $n$ possible cache
        locations (store up to 16 words with addresses xxxx1101 at cache
        location 1101).
      \end{itemize}
      Higher associativity is more expensive.
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Teaser}

  We have $N = 10^6$ two-dimensional coordinates, and want their centroid.
  Which of these is faster and why?
  \begin{enumerate}
  \item
    Store an array of $(x_i, y_i)$ coordinates.  Loop $i$ and simultaneously
    sum the $x_i$ and the $y_i$.
  \item
    Store an array of $(x_i, y_i)$ coordinates.  Loop $i$ and sum the $x_i$,
    then sum the $y_i$ in a separate loop.
  \item
    Store the $x_i$ in one array, the $y_i$ in a second array.  Sum the
    $x_i$, then sum the $y_i$.
  \end{enumerate}
  Let's see!

\end{frame}


\begin{frame}
  \frametitle{Caches on my laptop (I think)}

  \begin{itemize}
  \item 32 KB L1 data and memory caches (per core), \\
    8-way associative
  \item 256 KB L2 cache (per core), \\
    8-way associative
  \item 6 MB L3 cache (shared by all cores)
  \end{itemize}
\end{frame}


\begin{frame}[fragile]
  \frametitle{A memory benchmark (membench)}

\begin{verbatim}
  for array A of length L from 4 KB to 8MB by 2x
    for stride s from 4 bytes to L/2 by 2x
    time the following loop
      for i = 0 to L by s
        load A[i] from memory
\end{verbatim}

\end{frame}

\begin{frame}
  \frametitle{membench on my laptop -- what do you see?}

  \begin{center}
    \includegraphics[width=\textwidth]{lec03membench.pdf}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{membench on my laptop -- what do you see?}

  \begin{center}
    \includegraphics[width=0.9\textwidth]{lec03membench2.pdf}
  \end{center}
  \begin{itemize}
    \item Vertical: 64B line size ($2^5$), 4K page size ($2^{12}$)
    \item Horizontal: 32K L1 ($2^{15}$), 256K L2 ($2^{18}$), 6 MB L3
    \item Diagonal: 8-way cache associativity, 512 entry L2 TLB
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The moral}
  
  Even for simple programs, performance is a complicated function of
  architecture!
  \begin{itemize}
  \item Need to understand at least a little to write fast programs
  \item Would like simple models to help understand efficiency
  \item Would like common tricks to help design fast codes
    \begin{itemize}
    \item Example: {\em blocking} (also called {\em tiling})
    \end{itemize}
  \end{itemize}
\end{frame}


\end{document}
